import os
import glob
import time
import keras
from PIL import Image
from os import listdir
from shutil import copyfile
from os.path import isfile, join
from matplotlib import pyplot as plt

#from keras_vggface.utils import preprocess_input
#from keras_vggface.vggface import VGGFace 
#from keras.engine import Input
#import tensorflow.python.keras.engine
#from keras.utils.layer_utils import get_source_inputs
from keras.applications.vgg16 import VGG16
from keras.models import Model
import tensorflow as tf
import numpy as np
from keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Flatten, Dense, Dropout
#from keras_vggface.utils import preprocess_input
from sklearn.metrics import classification_report, confusion_matrix
#from MLEXPS.MLEXPS import *
from keras import backend as K
import random

random.seed(42)
tf.random.set_seed(42)

# Providing more training examples within certain distributions of age, gender, and race will increase the model's accuracy.

Height = 224
Width  = 224
BatchSize = 24
lr_rate=.0015
Version = 5
load_model = False
model_path = ''
accuracy = 0
accuracyCount = 0
trainableCount = 30

def SaveModelImage(Model, Title):
    keras.utils.vis_utils.plot_model(Model, to_file=Title, show_shapes=True, show_layer_names=True)
    return
    
def Summary(Model):
    print(Model.summary())
    return
    
def resnet():
    BaseModel = applications.resnet50.ResNet50(weights= None, include_top=False, input_shape= (224,224,3))
    last_layer = BaseModel.get_layer('activation_49').output
    print('here')
    return model
 
def MakeModel(dlsize):
    BaseModel = VGG16()
    last_layer = BaseModel.get_layer('block1_conv1').output

    x = keras.layers.Flatten(name='flatten')(last_layer)

    x = keras.layers.Dense(128, kernel_regularizer = keras.regularizers.l2(l = 0.015), activation='relu')(x)
    x = keras.layers.Dropout(rate=.4, seed=42)(x)

    out = keras.layers.Dense(2, activation='softmax', name='classifier')(x)
    DerivedModel = keras.Model(BaseModel.input, out)

    # # Everything is trainingable
    # # Weights are used at init
    # for layer in DerivedModel.layers:
    #     layer.trainable = True
    #
    #
    # # Everything in the base model is frozen
    # # Only top layers are trainable
    # for layer in BaseModel.layers:
    #     layer.trainable = False

    # base
    for layer in DerivedModel.layers:
        layer.trainable = False
    for layer in DerivedModel.layers[-trainableCount:]:
        layer.trainable = True


    DerivedModel.compile(keras.optimizers.Adam(lr=lr_rate), loss='categorical_crossentropy', metrics=['accuracy'])
    return DerivedModel
  
  def clearWeights(model):
    weights = model.get_weights()
    for weight in weights:
        weight = K.zeros(weight.shape, dtype=np.float64)
    model.set_weights(weights)
    return model
    
  def preprocess_input_new(x):
    img = preprocess_input(keras.preprocessing.image.img_to_array(x), version = 2)
    return keras.preprocessing.image.array_to_img(img)
    
  class EarlyStoppingAtMinLoss(tf.keras.callbacks.Callback):

  def __init__(self, trainableCount=30):
    print('working')
    super(EarlyStoppingAtMinLoss, self).__init__()
    self.epochCount = []
    self.trainableCount = trainableCount
    self.max = 0

def on_train_begin(self, logs=None):
    self.accuracyCount = 0
    self.accuracy = 0
    
 def on_epoch_end(self, epoch, logs=None):
    self.max = len(self.model.layers)
    print("Ending Epoch")
    if logs['val_accuracy'] > self.accuracy:
        self.accuracy = logs['val_accuracy']
        self.accuracyCount = 0
    else:
        self.accuracyCount+=1

    if self.accuracyCount >= 10 * (len(self.epochCount)+1):
        self.epochCount.append(epoch)
        print('Adding train layers')
        self.accuracyCount = 0
        self.trainableCount += 10
        if self.trainableCount >= self.max:
            self.trainableCount = self.max
        for layer in self.model.layers:
            layer.trainable = False
        for layer in self.model.layers[-self.trainableCount:]:
            layer.trainable = True
        self.model.compile(keras.optimizers.Adam(lr=lr_rate), loss='categorical_crossentropy', metrics=['accuracy'])
    print(self.epochCount)
    
    if __name__ == "__main__":
    timestr = time.strftime("%Y%m%d-%H%M%S")
    model = MakeModel(1024)
    # model = resnet()
    # model = clearWeights(model)
    model.compile(keras.optimizers.Adam(learning_rate=lr_rate), loss='categorical_crossentropy', metrics=['accuracy'])
    model.summary()
    
